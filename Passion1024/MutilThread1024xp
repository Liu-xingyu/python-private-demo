# coding=utf-8

'''
1024网站图片抓取并实现多线程下载
TODO 需要改良成多线程下载图片，提高效率，周末解决 2018-12-07
目前实现了多线程下载图片，效率很高，但还是存在一些小问题，图片文件保存的路径不是所属文件夹，后期有待优化
'''
import os
import socket
import time
from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED

import requests
from bs4 import BeautifulSoup


# 该函数用于下载图片
# 传入函数： 网页的网址url
def download_picture(url):
    url_preffix = 'http://1024.qsbtbgf.rocks/pw/'
    print(u'图片下载地址：', url)
    # 获取网页的源代码
    # r = requests.get(url)
    r = request(url, '1024.qsbtbgf.rocks')
    # 利用BeautifulSoup将获取到的文本解析成HTML
    soup = BeautifulSoup(r.text, "lxml")
    # 获取网页中的电影图片
    content = soup.find('div', class_='t z')
    all_a = content.find_all('h3')
    # 每一个小列表的地址、名称:
    url_name_list, url_link_list = [], []
    for a in all_a:
        url = a.find('a')
        a_name = url.get_text().encode('ISO-8859-1', 'ignore').decode('utf-8')
        a_link = url_preffix + url['href']
        url_name_list.append(a_name)
        url_link_list.append(a_link)
    # index = 1
    # for a in all_a:
    #     if index > 6:
    #         url = a.find('a')
    #         a_name = url.get_text().encode('ISO-8859-1', 'ignore').decode('utf-8')
    #         a_link = url_preffix + url['href']
    #         url_name_list.append(a_name)
    #         url_link_list.append(a_link)
    #     index += 1

    # 利用urllib.request..urlretrieve正式下载图片
    for picture_name, picture_link in zip(url_name_list, url_link_list):
        print(picture_name, u':%s' % picture_link)
        # mkDir(picture_name, 'D:\\1024xp_pictures\\portray001')
        mkDir(picture_name, 'D:\\1024xp_pictures\\passions')
        # mkDir(picture_name, 'D:\\1024xp_pictures\\sockLegs')
        getPicture(picture_link)


def getPicture(url):
    index = 0
    html = request(url, '1024.qsbtbgf.rocks')
    img_urls = BeautifulSoup(html.text, 'lxml').find('div', class_='tpc_content').find_all('img')
    real_urls = []
    for img_url in img_urls:
        index = index + 1
        src = img_url['src']
        real_urls.append(src)
        print(u'第%d' % index, '张图片,详细地址：%s' % src)
    # 图片保存着出现点问题，就是保存地址不匹配，实现了多线程的下载
    executor = ThreadPoolExecutor(max_workers=10)
    tasks = [executor.submit(saveFile, url) for url in real_urls]
    wait(tasks)


def mkDir(path, realIOPath):
    # realIOPath = "D:\portray001"
    spath = path.strip()
    isExists = os.path.exists(os.path.join(realIOPath, spath))
    if not isExists:
        print('建了一个名字叫做' + spath + '的文件夹！')
        os.makedirs(os.path.join(realIOPath, spath))
        os.chdir(os.path.join(realIOPath, spath))  # #切换到目录
        return True
    else:
        print('名字叫做' + spath + '的文件夹已经存在了！')
        return False


''' 保存图片到文件夹 '''


def saveFile(img_url):
    name = img_url[-12:-4]
    try:
        img = request(img_url, '')
        f = open(name + '.jpg', 'ab')
        f.write(img.content)
    except socket.timeout or FileNotFoundError or IOError or ConnectionError or ConnectionRefusedError or ConnectionAbortedError:
        pass
    finally:
        f.close()


# 记录文件下载记录
# def Schedule(a, b, c):
#     '''''
#     a:已经下载的数据块
#     b:数据块的大小
#     c:远程文件的大小
#    '''
#     per = 100.0 * a * b / c
#     if per > 100:
#         per = 100
#     print('%.2f%%' % per)


def main():
    # 全部10个网页
    start_urls = []
    for i in range(4, 8):
        start_urls.append("http://1024.qsbtbgf.rocks/pw/thread-htm-fid-16-page-%d.html" % i)

    # 统计该爬虫的消耗时间
    print('*' * 50)
    t3 = time.time()

    # 利用并发下载电影图片
    executor = ThreadPoolExecutor(max_workers=10)  # 可以自己调整max_workers,即线程的个数
    # submit()的参数： 第一个为函数， 之后为该函数的传入参数，允许有多个
    # 多线程传入的多个标题的地址，item_url
    future_tasks = [executor.submit(download_picture, url) for url in start_urls]
    # 等待所有的线程完成，才进入后续的执行
    wait(future_tasks, return_when=ALL_COMPLETED)

    t4 = time.time()
    print('使用多线程，总共耗时：%s' % (t4 - t3))
    print('*' * 50)


def request(url, host):
    # 在下载获取图片流内容时候没有host，需要将host置空，所以分开处理
    if host.strip() == '':
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36',
            'Referer': 'http://h3.cnmbtgf.xyz/pw/'
        }
    else:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36',
            'Referer': 'http://h3.cnmbtgf.xyz/pw/',
            'Host': host
        }
    try:
        content = requests.get(url, headers=headers)
        return content
    except FileNotFoundError or socket.timeout:
        pass


main()
